{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a363ca7-057f-435a-b0ea-f61f07b2e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import json\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3f6738-f228-4fc9-b41c-e58b375718a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\lemon\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lemon\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List three imaginative objects for a fantasy painting:\n",
      "- A magical forest with towering trees, lush greenery, and a river flowing through it.\n",
      "- A castle with a moat, turrets, and a castle keep.\n",
      "- A mystical lake\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_data = load_dataset(\"json\", data_files=\"train_data.json\", split=\"train\")\n",
    "test_data = load_dataset(\"json\", data_files=\"test_data.json\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bde05e3-f429-41e3-8cf1-4642bb8c118d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use the TinyLlama model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75721207-0045-47d3-8ec0-15b6076c5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give one random art style from historical periods, art style from animation, or famous artists from the past or on the internet for an artist to utilize. Only list the name, no description:\n",
      "1. Van Gogh - Starry Night (Orange Wheatfield With Crows)\n",
      "2. Matisse- L'oeil de Fontainebleau (The Eye of Fontevraud)\n",
      "3. Botticelli - Venus and Mars (Night Land\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['prompt'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = tokenizer(examples['completion'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bdcaa82b-6864-4227-a185-e97d29a90fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 5 well-known fantasy artists on Instagram. Only include their names or Instagram handles, one per line. No other information.\n",
      "- \"Fantasy Artists #3\" - A post featuring 20+ images of your favorite fantasy artists from Instagram, including biographies and links to the artist's portfolio pages (linking only works you personally admire). Be sure to include your own\n"
     ]
    }
   ],
   "source": [
    "prompt = \"List 5 well-known fantasy artists on Instagram. Only include their names or Instagram handles, one per line. No other information.\"\n",
    "\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=60,      # Number of new tokens to generate\n",
    "    temperature=0.95,        # Higher for randomness\n",
    "    top_p=0.95,             # Controls diversity\n",
    "    do_sample=True,         # Enables randomness instead of greedy decoding\n",
    "    num_return_sequences=1, # Generates 3 completions\n",
    "    repetition_penalty=1.2, # Penalizes repetitive text\n",
    "    early_stopping=False     # Stops early if end token is generated\n",
    ")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbe117c5-bf92-418e-a13e-b9eb4957ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a sentence that would provoke artistic inspiration:\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    'Generate a sentence that would provoke artistic inspiration:\\n'\n",
    ")\n",
    "output = generator(prompt, max_new_tokens=60, temperature=0.9, top_p=0.95)\n",
    "generated_text = output[0][\"generated_text\"]\n",
    "generated_text = generated_text.split(\"\\n\")[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf059ac-a48c-4a1f-9676-92fe96534396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "test_env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
